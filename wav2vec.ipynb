{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iemeuv-jKR3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0224fd-d51e-4fe8-896b-68360036a1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n",
            "Sun Sep  4 14:01:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 59.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 68.3 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!nvidia-smi\n",
        "!pip install -q transformers\n",
        "!pip install -q pydub\n",
        "!pip install torch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JAVqjOwJB96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d293151-69d6-4ac1-fc1f-8938d35e1d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "import librosa\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jNlOkFzJQAA"
      },
      "outputs": [],
      "source": [
        "def load_wav2vec_960h_model():\n",
        "  \"\"\"\n",
        "  Returns the tokenizer and the model from pretrained tokenizers models\n",
        "  \"\"\"\n",
        "  tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "  model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")    \n",
        "  return tokenizer, model\n",
        "\n",
        "def correct_uppercase_sentence(input_text): \n",
        "  \"\"\"\n",
        "  Returns the corrected sentence\n",
        "  \"\"\"  \n",
        "  sentences = nltk.sent_tokenize(input_text)\n",
        "  return (' '.join([s.replace(s[0],s[0].capitalize(),1) for s in sentences]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx2JRqoKXqph"
      },
      "outputs": [],
      "source": [
        "def asr_transcript(tokenizer, model, input_file):\n",
        "  \"\"\"\n",
        "  Returns the transcript of the input audio recording\n",
        "\n",
        "  Output: Transcribed text\n",
        "  Input: Huggingface tokenizer, model and wav file\n",
        "  \"\"\"\n",
        "  #read the file\n",
        "  speech, samplerate = sf.read(input_file)\n",
        "  #make it 1-D\n",
        "  if len(speech.shape) > 1: \n",
        "      speech = speech[:,0] + speech[:,1]\n",
        "  #Resample to 16khz\n",
        "  if samplerate != 16000:\n",
        "      speech = librosa.resample(speech, samplerate, 16000)\n",
        "  #tokenize\n",
        "  input_values = tokenizer(speech, return_tensors=\"pt\").input_values\n",
        "  #take logits\n",
        "  logits = model(input_values).logits\n",
        "  #take argmax (find most probable word id)\n",
        "  predicted_ids = torch.argmax(logits, dim=-1)\n",
        "  #get the words from the predicted word ids\n",
        "  transcription = tokenizer.decode(predicted_ids[0])\n",
        "  #output is all uppercase, make only the first letter in first word capitalized\n",
        "  transcription = correct_uppercase_sentence(transcription.lower())\n",
        "  return transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nGUkU5YsitH",
        "outputId": "8b554062-1b7f-4380-8b35-e4004dbfff41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
            "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you get it\n"
          ]
        }
      ],
      "source": [
        "wav_input = 'hello_world.wav'\n",
        "tokenizer, model = load_wav2vec_960h_model()\n",
        "text = asr_transcript(tokenizer,model,wav_input)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "9WkmjCc60h2k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY6II_ROhXK0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}